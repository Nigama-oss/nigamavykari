---
layout: post
title: "Training An AI To Play Chess"
categories: ml
author:
- Nigama Vykari 
---

The first and obvious step to create chess-ai is to have a labelled database that contains a lot of observations on real chess positions know which side is winning. Our algorithm needs to learn which move is good, and which one is bad. Luckily, python has a library called `'python-chess'` to help us achieve this. Install this with the following command.

```python
pip install python-chess==0.31.3
```

#### **Creating The Database**

To create the database, we have two functions. First, we need to create a function that gives us random chess positions playing random moves.

```python
#importing libraries
import chess
import chess.engine
import numpy
import random

#creating the board
def random_board(max_depth=200):
    board = chess.Board()
    depth = random.randrange(0, max_depth)
    
    for _ in range(depth):
        all_moves = list(board.legal_moves)
        random_move = random.choice(all_moves)
        board.push(random_move)
        if board.is_game_over():
            break
    return board
```

Second, we have to create a function that will create a program called 'stock fish' to give us the approximate score of each position.

```python
#creating the score
def stockfish(board, depth):
    with chess.SimpleEngine.popen_uci('/content/stockfish') as sf:
        result = sf.analyse(boar, chess.engine.Limit(depth=depth))
        score = result['score'].white().score()
        return score
```

Now, lets see how a random position looks like -

```python
board = random_board()
board
```

My results looked some thing like this. It might be different for you if you run the same code.

![](/assets/images/chess.png)

We can now see the scores generated by stockfish after analyzing these random positions. The score I got was -5430.

```python
print(stockfish(board, 10))
```

The value '10' in the above line is the depth. The more we increase the depth value, the more accurate score we get.

The next step is to convert the chess board into something that a neural network can understand and learn. And the way we do that is by representing the entire board with numbers.

```python
squares_index = {
  'a': 0,
  'b': 1,
  'c': 2,
  'd': 3,
  'e': 4,
  'f': 5,
  'g': 6,
  'h': 7
}
```

Now that we have defined them, its time to create some functions that can identify these positions on the chess board.

```python
# example: h3 -> 17
def square_to_index(square):
  letter = chess.square_name(square)
  return 8 - int(letter[1]), squares_index[letter[0]]


def split_dims(board):
  # this is the 3d matrix
  board3d = numpy.zeros((14, 8, 8), dtype=numpy.int8)

  # here we add the pieces's view on the matrix
  for piece in chess.PIECE_TYPES:
    for square in board.pieces(piece, chess.WHITE):
      idx = numpy.unravel_index(square, (8, 8))
      board3d[piece - 1][7 - idx[0]][idx[1]] = 1
    for square in board.pieces(piece, chess.BLACK):
      idx = numpy.unravel_index(square, (8, 8))
      board3d[piece + 5][7 - idx[0]][idx[1]] = 1

  # add attacks and valid moves too
  # so the network knows what is being attacked
  aux = board.turn
  board.turn = chess.WHITE
  for move in board.legal_moves:
      i, j = square_to_index(move.to_square)
      board3d[12][i][j] = 1
  board.turn = chess.BLACK
  for move in board.legal_moves:
      i, j = square_to_index(move.to_square)
      board3d[13][i][j] = 1
  board.turn = aux

  return board3d
```

These functions convert the board into 14 / 8 / 8 matrix. This results in an array of matrix values of ones and zeroes representing black and white pieces respectively. You will also have two matrices that shows all the positions attacked by black and vice versa (also represented in ones and zeroes). This information is crucial for our neural network to perform well in training.

It is now time to involve tensorflow and model our network. To create the model we define a function `'build_model'`.

```python
import tensorflow.keras.models as models
import tensorflow.keras.layers as layers
import tensorflow.keras.utils as utils
import tensorflow.keras.optimizers as optimizers


def build_model(conv_size, conv_depth):
  board3d = layers.Input(shape=(14, 8, 8))

  # adding the convolutional layers
  x = board3d
  for _ in range(conv_depth):
    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', activation='relu', data_format='channels_first')(x)
  x = layers.Flatten()(x)
  x = layers.Dense(64, 'relu')(x)
  x = layers.Dense(1, 'sigmoid')(x)

  return models.Model(inputs=board3d, outputs=x)
 ```

We now want to see the different layers of the neural network to understand how this is going to work -

```python
model = build_model(32, 4)
utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=False)
```

To improve the model or make deeper connections in the network, we could use residual networks. Try the code below, if you want to test a residual model.

```python
def build_model_residual(conv_size, conv_depth):
  board3d = layers.Input(shape=(14, 8, 8))

  # adding the convolutional layers
  x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(board3d)
  for _ in range(conv_depth):
    previous = x
    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, previous])
    x = layers.Activation('relu')(x)
  x = layers.Flatten()(x)
  x = layers.Dense(1, 'sigmoid')(x)

  return models.Model(inputs=board3d, outputs=x)
```

And it is now time for training the model. I have trained the model on 2 gigabytes of [dataset](https://drive.google.com/file/d/1YcFh-uBHflrRuQjh3rQ8CFfY2YKx7ytw/edit)containing labels and boards. This must be more than enough for the network to train on.

```python
import tensorflow.keras.callbacks as callbacks


def get_dataset():
	container = numpy.load('dataset.npz')
	b, v = container['b'], container['v']
	v = numpy.asarray(v / abs(v).max() / 2 + 0.5, dtype=numpy.float32) # normalization (0 - 1)
	return b, v


x_train, y_train = get_dataset()
print(x_train.shape)
print(y_train.shape)

model.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')
model.summary()
model.fit(x_train, y_train,
          batch_size=2048,
          epochs=1000,
          verbose=1,
          validation_split=0.1,
          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),
                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])

model.save('model.h5')
```

Model training requires a lot of time. So, do not get worried if you don't see any results immediately. The model requires time to read the position, learn and then respond.

There are some good learning hits and some callbacks in the code to stop the training, when the model stops learning. Now, finally its time to see our model play.

```python
# used for the minimax algorithm
def minimax_eval(board):
  board3d = split_dims(board)
  board3d = numpy.expand_dims(board3d, 0)
  return model.predict(board3d)[0][0]


def minimax(board, depth, alpha, beta, maximizing_player):
  if depth == 0 or board.is_game_over():
    return minimax_eval(board)
  
  if maximizing_player:
    max_eval = -numpy.inf
    for move in board.legal_moves:
      board.push(move)
      eval = minimax(board, depth - 1, alpha, beta, False)
      board.pop()
      max_eval = max(max_eval, eval)
      alpha = max(alpha, eval)
      if beta <= alpha:
        break
    return max_eval
  else:
    min_eval = numpy.inf
    for move in board.legal_moves:
      board.push(move)
      eval = minimax(board, depth - 1, alpha, beta, True)
      board.pop()
      min_eval = min(min_eval, eval)
      beta = min(beta, eval)
      if beta <= alpha:
        break
    return min_eval


# this is the actual function that gets the move from the neural network
def get_ai_move(board, depth):
  max_move = None
  max_eval = -numpy.inf

  for move in board.legal_moves:
    board.push(move)
    eval = minimax(board, depth - 1, -numpy.inf, numpy.inf, False)
    board.pop()
    if eval > max_eval:
      max_eval = eval
      max_move = move
  
  return max_move
  
  board = chess.Board()

with chess.engine.SimpleEngine.popen_uci('/content/stockfish') as engine:
  while True:
    move = get_ai_move(board, 1)
    board.push(move)
    print(f'\n{board}')
    if board.is_game_over():
      break

    move = engine.analyse(board, chess.engine.Limit(time=1), info=chess.engine.INFO_PV)['pv'][0]
    board.push(move)
    print(f'\n{board}')
    if board.is_game_over():
      break
      
!cp "/content/drive/My Drive/dataset.zip" /content/dataset.zip
!unzip dataset.zip
!rm dataset.zip
!chmod +x stockfish

import random
random.seed(37)
```

**Note:** You have to understand that we are not telling the model what needs to be done, rather it is learning from its observations. We just make sure the model know what is a legal move, but do not encode the game itself.

There are a lot of improvements that needs to be done to improve the speed of our network, but it turned out better than I expected.



























